# nlp_attention
Some fun experiments around different types of NLP attention mechanisms

Paper:
- [Group Query Attention](https://arxiv.org/abs/2305.13245)
- [Sliding Window Attention](https://arxiv.org/abs/2004.05150)
- [StreamingLLM](https://arxiv.org/abs/2309.17453)

TODO:
- [Shift Short Attention](http://arxiv.org/abs/2309.12307)

Credit: [Rishikesh Magar](https://github.com/RishikeshMagar)
